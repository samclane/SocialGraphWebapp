{% load graph_extras %}
<!DOCTYPE html>
<style>
    .content {
        margin: auto;
        max-width: 960px;
    }

    h1 {
        text-align: center;
    }

    p {
        text-align: center;
    }

    iframe {
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    dl {
        text-align: center;
    }

    dt {
        font-weight: bold;
    }

    table.center {
        border-collapse: collapse;
        margin-left: auto;
        margin-right: auto;
    }

    td, th {
        border: 1px solid #dddddd;
        text-align: left;
        padding: 3px;
    }

    tr:nth-child(even) {
        background-color: #dddddd;
    }

    .top-border {
        border-top: 2px black solid;
    }

</style>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Discord Social Graph</title>
</head>
<body>
    <div class="content">
        <header>
            <h1>Discord Social Graph</h1>
            <p>
                An ongoing study on social data collection, processing, and analysis by Sawyer McLane.<br/>
                Source code can be found
                <a href="https://github.com/samclane/SocialGraphWebapp">here.</a>
            </p>
        </header>
        <div class="graphs">
            <iframe src="{% url "graphs" %}" style="width:640px;height:500px;border:none" scrolling="no"></iframe>
            <p class="subtitle">The graph on the left is the titular Discord Social Graph. Originally, the graph is a fully-connected bidirectional weighted graph. The weight from person A to person B is calculated by multiplying P<sub>1</sub> (the probability A will join the server given only person B is present) by N<sub>B</sub> (the number of times person B appears in the data). The graph is arranged according to the <a href="https://github.com/gephi/gephi/wiki/Fruchterman-Reingold">Fruchterman Reingold layout algorithm.</a><br/><hr/>
            The graph on the right, called the Receiver Operating Characteristic (ROC) is used to determine the accuracy of a binary classifier as the classification threshold is increased. The x-axis is false-positives, that is, % of testing samples that were misclassified. The y-axis is the true-positives, the labels the classifier gets right. A perfect classifier would immediately go to 1.0 then stay there as the graph moved right. A completely random classifier would be a diagonal line (50/50 chance of getting it right). A worst-case classifier would be a flat 0.0, indicating the classifier is doing <i>worse</i> than random.</p>
        </div>
        <div class="metrics">
            <h1>Popularity List</h1>
            <p class="subtitle">The popularity list is the real meat of the project. A member's "popularity" is determined by the sum of the in-degree weights.</p>
            <p class="metric">{{ popularity_list|safe|linebreaks|spacify }}</p>
            <h1>Cross Validation</h1>
            <p class="subtitle">Cross-validation splits the data into training and testing sets,  and gives the % correct the classifier gets in the testing set. The data is shuffled and split 5 times.</p>
            <p class="metric">{{ cross_val|safe|linebreaks|spacify }}</p>
            <h1>Accuracy</h1>
            <p class="subtitle">Accuracy is the overall percentage of labels the classifier gets right on the entire dataset.</p>
            <p class="metric"><b>{{ accuracy|safe|linebreaks|spacify }}</b></p>
            <h1>Classification Report</h1>
            <p class="subtitle">The classification report gives us specific information on how the classifier fares on each class.</p>
            <dl>
                <dt>Precision</dt>
                <dd>Given the classifier chooses class C, the % of times it's correct.</dd>
                <dt>Recall</dt>
                <dd>% of times class C is chosen correctly in the entire dataset.</dd>
                <dt>F1-Score</dt>
                <dd>The "harmonic mean" of Precision and Recall</dd>
                <dt>Support</dt>
                <dd>The number of occurrences of class C in the data.</dd>
            </dl>
            <p class="metric">{{ class_report|tablify_report }}</p>
            <h1>Confusion Matrix</h1>
            <p class="subtitle">Misclassifications for each member. Currently not formatted quite well; each column in the matrix corresponds to a member, in the same order as the rows. An ideal classifier's confusion matrix would be 0 except for the principal diagonal.</p>
            <p class="metric">{{ conf_matrix|safe|linebreaks|spacify }}</p>
        </div>
    </div>
</body>
</html>